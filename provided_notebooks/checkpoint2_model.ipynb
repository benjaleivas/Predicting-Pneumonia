{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFAQCELc8ltp"
      },
      "outputs": [],
      "source": [
        "## CHECKPOINT #2\n",
        "\n",
        "# There are three objectives for Checkpoint #2: \n",
        "#   - identify relevant metrics (2+) for assessing classifier accuracy\n",
        "#   - build a custom Neural Network:\n",
        "#       * with at least one input layer, one hidden layer, one output layer\n",
        "#   - initial analysis of this custom model\n",
        "#       * which would require a complete run through of trainning, validating \n",
        "#         testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO5vVib5_TUR"
      },
      "outputs": [],
      "source": [
        "# FIRST OBJECTIVE\n",
        "\n",
        "# Choosing the relevant metrics is up to you. There are few things for things\n",
        "# for you to consider.\n",
        "\n",
        "# In choosing relevant metrics, I recommend considering at least one ACCURACY\n",
        "# METRIC AND at leat one LOSS FUNCTION! \n",
        "# 1) measure of accuracy: \n",
        "#   sci-kit learn's page on this could be useful for you:\n",
        "#   https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
        "#   there are many possible metrics to choose from: accuracy score, F1 score, \n",
        "#   precision-recall curve, etc\n",
        "# 2) loss functions:\n",
        "#   In class, we've seen Negative Log Loss, which works well for a binary\n",
        "#   or a multi-class classification. There is also cross-entropy, which is\n",
        "#   widely used for binary classifcation. There are other loss functions, too!\n",
        "#   This blog post is a good starting point: https://neptune.ai/blog/pytorch-loss-functions\n",
        "\n",
        "# CONSIDERATION: though this objective is listed first, I recommend coming to this\n",
        "# after you built your model. As you build the train/val/test process, assessing\n",
        "# accuracy and loss will be done pretty simply, often with a call of a single\n",
        "# function/method for each metric. Start with the metrics you've seen in class\n",
        "# and then consider branching out! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynyYL4DbCNwT"
      },
      "outputs": [],
      "source": [
        "# SECOND OBJECTIVE\n",
        "\n",
        "# Building a custom neural network! The exciting part <3\n",
        "\n",
        "# FYI: HW4 will be a great introduction to using PyTorch to build a simple\n",
        "# neural network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61pG2bSzZuMe"
      },
      "outputs": [],
      "source": [
        "from torchvision.io import read_image\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3NrRai5Z-iz",
        "outputId": "8aa885a4-ffe2-429c-b94a-20c2b8aa2053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPJe6akSEhFK"
      },
      "outputs": [],
      "source": [
        "# import our libraries\n",
        "import torch \n",
        "import torch.nn as nn # basic building block for neural neteorks\n",
        "import torch.nn.functional as F # import convolution functions like Relu\n",
        "import torch.optim as optim # optimzer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-EvHfmIZ_lO"
      },
      "outputs": [],
      "source": [
        "# 1: make your model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QN6b86UE0Y2"
      },
      "outputs": [],
      "source": [
        "# MY MODEL EXPECTS AN 512 x 512 IMAGE with dtype = float32\n",
        "\n",
        "class CustomNeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "        # inspire by Turing award winning LeCun, Bengio and Hinton's paper from 1998\n",
        "        # https://ieeexplore.ieee.org/document/726791 (cited more than 25,000 times!!!!!!!!!)\n",
        "        # code from https://blog.paperspace.com/writing-lenet5-from-scratch-in-python/ \n",
        "        self.LeNet = nn.Sequential(     \n",
        "            # convolutional layers            \n",
        "            nn.Sequential(                                            # FIRST LAYER: (INPUT LAYER)\n",
        "              nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=0),    # CONVOLUTION \n",
        "              nn.BatchNorm2d(6),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool2d(kernel_size = 2, stride = 2)),             # POOLING\n",
        "            nn.Sequential(                                            # SECOND LAYER: HIDDEN LAYER 1\n",
        "              nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0),   # CONVOLUTION \n",
        "              nn.BatchNorm2d(16),\n",
        "              nn.ReLU(),\n",
        "              nn.MaxPool2d(kernel_size = 2, stride = 2)),             # POOLING\n",
        "            # fully connected layers\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(250000, 120),                                   # THIRD LAYER: LINEAR YEAR, HIDDEN LAYER 2\n",
        "            nn.ReLU(),                                                # HIDDEN LAYER's ACTIVATION FUNCION\n",
        "            nn.Linear(120, 84),                                       # FOURTH LAYER: LINEAR YEAR, HIDDEN LAYER 3\n",
        "            nn.ReLU(),                                                # HIDDEN LAYER's ACTIVATION FUNCION\n",
        "            # output layer\n",
        "            nn.Linear(84, 2)                                          # OUTPUT LAYER\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.LeNet(x)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyUHBABjPcGh"
      },
      "outputs": [],
      "source": [
        "# RESOURCE FOR CHOOSING ACTIVATION FUNCTIONS\n",
        "# https://towardsdatascience.com/how-to-choose-the-right-activation-function-for-neural-networks-3941ff0e6f9c\n",
        "\n",
        "# RESOURCES ON CONVOLUTION: \n",
        "# NOTE: not necessary to implemenent a convolutional layer. this is helpful\n",
        "# IF you want to implement your own costum convolutional layer!\n",
        "# http://www.songho.ca/dsp/convolution/convolution.html#convolution_2d\n",
        "# http://www.songho.ca/dsp/convolution/convolution2d_example.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomNeuralNetwork()"
      ],
      "metadata": {
        "id": "y0fdTAlkBKzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2: get your dataloaders from the first checkpoint!\n",
        "train_loader = #SOME CODE\n",
        "val_loader = #SOME CODE\n",
        "test_loader = #SOME CODE"
      ],
      "metadata": {
        "id": "ACtoJg80E1IZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3: Define a Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "Ap8J5z4eC4PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4: Train and validate the network\n",
        "EPOCHS = 50\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuarcies = []\n",
        "\n",
        "for _ in range(EPOCHS):  # loop over the dataset multiple times\n",
        "\n",
        "    # TRAIN\n",
        "    # Make sure gradient tracking is on, and do a pass over the data\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data           # NOTE: depending on how you implemented your dataset class's __getitem__ it could be labels, inputs\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = model(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # keep track of the loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # ALSO CALCULATE YOUR ACCURACY METRIC\n",
        "      \n",
        "    avg_train_loss = running_loss / (i + 1)     # i + 1 gives us the total number of batches in train dataloader\n",
        "    # CALCULATE AVERAGE ACCURACY METRIC\n",
        "    avg_train_loss = \n",
        "    train_losses.append(avg_train_loss)\n",
        "    train_accuracies.append(avg_train_acc)\n",
        "\n",
        "    #VALIDATE\n",
        "    # in the validation part, we don't want to keep track of the gradients \n",
        "    model.eval()            \n",
        "    \n",
        "    # implement a similar loop!\n",
        "    # but you can leave out loss.backward()"
      ],
      "metadata": {
        "id": "xPfEI2rRDPnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5: Test!\n",
        "\n",
        "# FOR TESTING YOU DON'T HAVE TO ITERATE OVER MULTIPLE EPOCHS\n",
        "# JUST ONE PASS OVER THE TEST DATALOADER!\n"
      ],
      "metadata": {
        "id": "DXRfmBsiLmUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6: ANAYLZE (i.e. 3RD OBJECTIVE)\n",
        "\n",
        "# YOU CAN MAKE GRAPHS of TRAIN AND VAL LOSSES OVER EPOCHS, etc!\n",
        "# YOU CAN ALSO DO MULTIPLE TRAININGS, CHOOSING A DIFFERENT LOSS FUNCTION\n",
        "# FOR EACH TRAINING RUN, AND THEN YOU COULD COMPARE HOW WHICH LOSS FUNCTION\n",
        "# LEADS TO THE BEST LOSSES OR BEST ACCURACIES\n",
        "\n",
        "# ALSO YOU COULD TRAIN USING DIFFERENT OPTIMIZERS!\n",
        "\n",
        "# SO MUCH YOU COULD DO!"
      ],
      "metadata": {
        "id": "GJfuHA1fM-ve"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}